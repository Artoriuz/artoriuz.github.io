<!DOCTYPE html>
<html lang="en">

<head>
	<title>Digital Image Processing</title>
	<meta charset="utf-8" />
	<link rel="stylesheet" type="text/css" href="./css/default.css" />
	<link rel="stylesheet" type="text/css" href="./css/syntax.css" />
	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=doxy"></script>
</head>

<body>
	<header>
		<hgroup>
			<h1>
				<a href="./image_processing.html">Digital Image Processing</a>
			</h1>
			<h2>João Vitor Rafael Chrisóstomo</h2>
		</hgroup>
	</header>
	<nav>
		<menu>
			<a href="./index.html">Home</a>
            <a href="https://github.com/artoriuz">GitHub</a>
            <a href="./image_processing_pt.html">Versão em português</a>
		</menu>
	</nav>
	<section>
		<section id="atividades">
			<h2>Algorithms</h2>
			<a href="#negativo"><li><h4>Negative</h4></li></a>
			<a href=#quadrantes><li><h4>Quadrants</h4></li></a>
			<a href=#contagem><li><h4>Object Counting</h4></li></a>
			<a href=#equalização><li><h4>Histogram Equalisation</h4></li></a>
			<a href=#movimentos><li><h4>Movement Detection</h4></li></a>
			<a href=#lapgauss><li><h4>Spacial-domain filtering</h4></li></a>
			<a href=#tiltshift><li><h4>Tilt-shift</h4></li></a>
		</section>
		<section id="posts">
			<br>
			<br>
			<h3 id="negativo">Negative</h3>
            <p>
                The first algorithm simply performs a colour inversion in a given area and outputs the result. To understand the procedure it's important to understand 
                how RGB digital images store colour information. When the sub-pixel matrix consists of 3 different independent colour channels (red, blue and green), 
                a combination of intensity of those channels results in the perceived output colour.     
            </p>
            <p>
                Multiple different pixel format standards exist, but if your image is targetting the web it's sane to assume we have 8 bits of precision per colour channel.
                Which gives us 24 bits per pixel or 16777216 different possible colours.
            </p>
            <p>
                The colour "white" is an equal combination of all colours, so in our system to output white we need maximum intensity in all three of our colour channels,
                which means white is (255, 255, 255) in decimal, or #FFFFFF in hexadecimal. 
            </p>
			<p>
                The inverse of a colour is simply the difference between absolute white and the colour itself, and since the pixel data is stored in binary we can perform
                a bitwise_not in order to invert the value of all the bits (equivalent of subtracting their original value from 255).
            </p>
			<p>Here we can see our original image:</p>
			<p>
				<img src="./images/inverse/reina.png" alt="Original Reina" class="center">
			</p>
			<p>The following program receives the coordinates from the user, and then performs the inversion:</p>
			<pre class="prettyprint">
	<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace std;
using namespace cv;

int main(int argc, char** argv) {

	Mat img = imread(argv[1]);
	if (!image.data) {
		cout << "imagem nao carregou corretamente" << endl;
		return(-1);
	}
	
	Mat img_out = Mat::zeros(img.rows, img.cols, CV_8UC3);

	int pos_xi, pos_yi, pos_xf, pos_yf;

	cout << "digite o pixel x inicial:" << endl;
	cin >> pos_xi;
	cout << "digite o pixel y inicial:" << endl;
	cin >> pos_yi;

	cout << "digite o pixel x final:" << endl;
	cin >> pos_xf;
	cout << "digite o pixel y final:" << endl;
	cin >> pos_yf;

	namedWindow("janela", WINDOW_AUTOSIZE);
	imshow("janela", img);
	waitKey();


	for (int i = 0; i < img.rows; i++) {
		for (int j = 0; j < img.cols; j++) {
			if (i > pos_xi && i < pos_xf && j > pos_yi && j < pos_yf) {
				bitwise_not(img.at<Vec3b>(i, j), img_out.at<Vec3b>(i, j));
			}
			else {
				img_out.at<Vec3b>(i, j) = img.at<Vec3b>(i, j);
			}
		}
	}
	imshow("janela", img);

	namedWindow("janelaout", WINDOW_AUTOSIZE);
	imshow("janelaout", img_out);

	waitKey();
	return 0;
}

    </code>
	</pre>
	<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/inverse.cpp">inverse.cpp</a>
			<p>We can see the resulting image here:</p>
			<p>
				<img src="./images/inverse/inverted_reina.png" alt="Inverted Reina" class="center">
			</p>
			<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
			<h3 id="quadrantes">Quadrants</h3>
			<p>
                An image can be divided into 4 quadrants, the following example aims to swap the position of those quadrants maintaining the original pixel values.  
            </p>
			<p>Here we can see the original quadrant configuration of any image:</p>
			<p>
				<img src="./images/quadrants/correct.png" alt="Original Quadrants" class="center">
			</p>
			<p>As requested, the inversion of quadrants should be done in the following manner:</p>
			<p>
				<img src="./images/quadrants/inverted.png" alt="Inverted Quadrants" class="center">
			</p>
			<p>And finally, we can see the program that does that for us:</p>
			<pre class="prettyprint">
	<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace std;
using namespace cv;

int main(int argc, char** argv) {

	Mat img = imread(argv[1]);
	if (!image.data) {
		cout << "imagem nao carregou corretamente" << endl;
		return(-1);
	}

	Mat img_out = Mat::zeros(img.rows, img.cols, CV_8UC3);

	namedWindow("janela", WINDOW_AUTOSIZE);
	imshow("janela", img);
	waitKey();

	for (int i = 0; i < img.rows; i++) {
		for (int j = 0; j < img.cols; j++) {
			if (i < img.rows / 2 && j < img.cols / 2) {
				img_out.at<Vec3b>(i + img.rows / 2, j + img.cols / 2) = img.at<Vec3b>(i, j);
			}
			else if (i < img.rows / 2 && j > img.cols / 2) {
				img_out.at<Vec3b>(i + img.rows / 2, j - img.cols / 2) = img.at<Vec3b>(i, j);
			}
			else if (i > img.rows / 2 && j < img.cols / 2) {
				img_out.at<Vec3b>(i - img.rows / 2, j + img.cols / 2) = img.at<Vec3b>(i, j);
			}
			else if (i > img.rows / 2 && j > img.cols / 2) {
				img_out.at<Vec3b>(i - img.rows / 2, j - img.cols / 2) = img.at<Vec3b>(i, j);
			}
		}
	}

	imshow("janela", img);

	namedWindow("janelaout", WINDOW_AUTOSIZE);
	imshow("janelaout", img_out);

	waitKey();
	return 0;
}

	</code>
	</pre>
	<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/quadrants.cpp">quadrants.cpp</a>
			<p>First let's see how the original image looked like:</p>
			<p>
				<img src="./images/quadrants/rin_correct.png" alt="Correct Rin" class="center">
			</p>
			<p>And how it looks now after we swapped its quadrants:</p>
			<p>
				<img src="./images/quadrants/rin_inverted.png" alt="Inverted Rin" class="center">
			</p>
			<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
			<h3 id="contagem">Object counting</h3>
			<p>Now we'll learn how to count objects, in order to increase the complexity of the algorithm and its accuracy, 
                it's important to differentiate objects with holes and objects without.</p>
			<p>The following image will be the input of the program:</p>
			<p>
				<img src="./images/count/bolhas.png" alt="Original image" class="center">
			</p>
            <p>
                The very first thing the program does is delete all the objects that are touching the borders of the image. After that, the background 
                is "painted" in a certain colour of choice in order to differentiate it from the holes. Then, the program will search for white pixels 
                and paint the entire agglomerate of white pixels with a given colour. This colour changes each time the program finds another agglomerate of
                white pixels, which are the objects.  
            </p>
            <p>
                After all the objects are found and painted with a corresponding colour, the program will search for holes and paint those holes with the same
                colour it used to paint the object which this hole is inside of, which prevents the program from counting objects with 2 holes wrongly.
            </p>
			<p>The program can be seen here:</p>
			<pre class="prettyprint">
	<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace std;
using namespace cv;

int main(int argc, char** argv) {
	
	Mat image = imread(argv[1], CV_LOAD_IMAGE_GRAYSCALE);
	if (!image.data) {
		cout << "imagem nao carregou corretamente" << endl;
		return(-1);
	}

	long int obj_total = 0;
	long int obj_with_holes = 0;
	long int obj_found_colour = 2;
	long int obj_colour_aux = 0;

	CvPoint p;

	namedWindow("Labelling", CV_WINDOW_KEEPRATIO);
	imshow("Labelling", image);
	waitKey();

	for (int i = 0; i < image.rows; i++) {
		for (int j = 0; j < image.cols; j++) {
			if (i == 0 || j == 0 || i == image.rows - 1 || j == image.cols - 1) {
				image.at<uchar>(i, j) = 255;
			}
		}
	}
	p.x = 0; p.y = 0;
	floodFill(image, p, 0); //Retirada das bolhas que tocam as bordas

	imshow("Labelling", image);
	waitKey();

	floodFill(image, p, 1); //Mudança do valor da cor de fundo

	imshow("Labelling", image);
	waitKey();

	for (int i = 0; i < image.rows; i++) { //Marca todos os objetos como sem bolha
		for (int j = 0; j < image.cols; j++) {
			if (image.at<uchar>(i, j) == 255) {
				obj_total++; //Achou algo
				p.x = j;
				p.y = i;
				floodFill(image, p, obj_found_colour);
				obj_found_colour++;
			}
		}
	}

	imshow("Labelling", image);
	waitKey();

	for (int x = 0; x < image.rows; x++) { //Fazer uma busca dos objetos com buracos
		for (int y = 0; y < image.cols; y++) {
			if (image.at<uchar>(x, y) == 0 && image.at<uchar>(x, y - 1) != obj_colour_aux) {
				obj_colour_aux = image.at<uchar>(x, y - 1);
				p.x = y;
				p.y = x;
				floodFill(image, p, obj_colour_aux);
				obj_with_holes++;
			}
			else if (image.at<uchar>(x, y) == 0 && image.at<uchar>(x, y - 1) == obj_colour_aux) {
				p.x = y;
				p.y = x;
				floodFill(image, p, obj_colour_aux);
			}
		}
	}

	imshow("Labelling", image);
	waitKey();

	imwrite("labeling.png", image);

	cout << "Number of elements without holes: " << obj_total - obj_with_holes << endl;
	cout << "Number of elements with holes: " << obj_with_holes << endl;

	waitKey();
	return 0;
}
			
			

	</code>
	</pre>
	<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/counting.cpp">counting.cpp</a>
			<p>Following the program, first we delete all the objects that touch the border:</p>
			<p>
				<img src="./images/count/bolhas_no_border_obj.png" alt="Border elements were removed" class="center">
			</p>
			<p>Then, we count the total number of objects while painting them:</p>
			<p>
				<img src="./images/count/bolhas_count_1.png" alt="Objects were found" class="center">
			</p>
			<p>And then, we count the holes taking into consideration to colour of the object:</p>
			<p>
				<img src="./images/count/bolhas_count_2.png" alt="Objects with holes were found" class="center">
			</p>
			<p>Finally, we can see the end result in our terminal:</p>
			<p>
				<img src="./images/count/bolhas_result.png" alt="Terminal text output" class="center">
			</p>
			<p>To make sure the program wouldn't count objects with multiple holes wrongly, a modification was done in the input image:</p>
			<p>
				<img src="./images/count/bolhas2.png" alt="Edited image" class="center">
			</p>
			<p>And then we can go through the same sequence of images following the code again:</p>
			<p>
				<img src="./images/count/bolhas2_count_1.png" alt="Objects were found" class="center">
			</p>
			<p>
				<img src="./images/count/bolhas2_count_2.png" alt="Objects with holes were found" class="center">
			</p>
			<p>We can check how many objects and objects with holes were counted in our terminal output, it's important to notice the numbers didn't change:</p>
			<p>
				<img src="./images/count/bolhas_result.png" alt="Terminal text output" class="center">
			</p>
			<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
			<h3 id="equalização">Histogram Equalisation</h3>
			<p>Our aim is to equalise the histogram of a greyscale image, maximising its dynamic range to the highest possible value using the existing greyscale values.
			</p>
			<p>This will be our input image:
			</p>
			<p>
				<img src="./images/equalisation/charlotte_original.png" alt="Original image" class="center">
			</p>
			<p>
				The first thing we need to do is to load the image as a greyscale image, using a property of OpenCV itself to do it:
			</p>
			<p>
				<img src="./images/equalisation/charlotte_greyscale.png" alt="Greyscale image" class="center">
			</p>
			<p>
				The program that calculates and plots the histograms of both the input image and its equalised output is shown below:
			</p>
			<p>
				<pre class="prettyprint">
					<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace cv;
using namespace std;

int main(int argc, char** argv) {
	Mat img, img_eql, hist, hist_eql;
	img = imread(argv[1], CV_LOAD_IMAGE_GRAYSCALE);

	equalizeHist(img, img_eql);

	bool uniform = true;
	bool acummulate = false;

	int nbins = 128;
	float range[] = { 0, 256 };
	const float *histrange = { range };

	int width = img.cols;
	int height = img.rows;

	int histw = nbins, histh = nbins / 4;

	Mat hist_img(histh, histw, CV_8UC1, Scalar(0, 0, 0));
	Mat hist_img_eql(histh, histw, CV_8UC1, Scalar(0, 0, 0));

	calcHist(&img, 1, 0, Mat(), hist, 1, &nbins, &histrange, uniform, acummulate);
	calcHist(&img_eql, 1, 0, Mat(), hist_eql, 1, &nbins, &histrange, uniform, acummulate);

	normalize(hist, hist, 0, hist_img.rows, NORM_MINMAX, -1, Mat());
	normalize(hist_eql, hist_eql, 0, hist_img.rows, NORM_MINMAX, -1, Mat());

	hist_img.setTo(Scalar(0));
	hist_img_eql.setTo(Scalar(0));

	for (int i = 0; i&#60;nbins; i++) {
		line(hist_img, Point(i, histh), Point(i, cvRound(hist.at&#60;float&#62;(i))), Scalar(255, 255, 255), 1, 8, 0);
	}
	for (int i = 0; i&#60;nbins; i++) {
		line(hist_img_eql, Point(i, histh), Point(i, cvRound(hist_eql.at&#60;float&#62;(i))), Scalar(255, 255, 255), 1, 8, 0);
	}

	hist_img.copyTo(img(Rect(10, 10, nbins, histh)));
	hist_img_eql.copyTo(img_eql(Rect(10, 10, nbins, histh)));

	namedWindow("Original", WINDOW_AUTOSIZE);
	namedWindow("Equalised", WINDOW_AUTOSIZE);

	imshow("Original", img);
	imshow("Equalised", img_eql);
	waitKey();

	return 0;
}
					</code>
				</pre>
			</p>
			<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/equalise.cpp">equalise.cpp</a>
			<p>
				The first plot is simply the original image in greyscale plus its histogram:
			</p>
			<p>
				<img src="./images/equalisation/charlotte_greyscale_histogram.png" alt="Greyscale image with its histogram" class="center">
			</p>
			<p>
				And now we can see the same plot, but for the equalised image:
			</p>
			<p>
				<img src="./images/equalisation/charlotte_greyscale_equalised_histogram.png" alt="Greyscale image with its histogram" class="center">
			</p>
			<p>
				The code was originally designed to be used with still images, however with some modifications it can be used for videos (or camera inputs, which are also video streams). 
			</p>
            <p>
                In the following example, a video was used intead of a camera input because of technical limitations (lack of a camera). It can be used with a camera
                swapping "argv[1]" in "VideoCapture" to "0". "0" opens the camera instead.
            </p>
			<p>
				<pre class="prettyprint">
					<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace cv;
using namespace std;

int main(int argc, char** argv) {
	Mat img, img_eql, hist, hist_eql;
	VideoCapture cap(argv[1]);
	while (true) {
		cap >> img;
		cvtColor(img, img, CV_BGR2GRAY);
		equalizeHist(img, img_eql);

		bool uniform = true;
		bool acummulate = false;

		int nbins = 128;
		float range[] = { 0, 256 };
		const float *histrange = { range };

		int width = img.cols;
		int height = img.rows;

		int histw = nbins, histh = nbins / 4;

		Mat hist_img(histh, histw, CV_8UC1, Scalar(0, 0, 0));
		Mat hist_img_eql(histh, histw, CV_8UC1, Scalar(0, 0, 0));

		calcHist(&img, 1, 0, Mat(), hist, 1, &nbins, &histrange, uniform, acummulate);
		calcHist(&img_eql, 1, 0, Mat(), hist_eql, 1, &nbins, &histrange, uniform, acummulate);

		normalize(hist, hist, 0, hist_img.rows, NORM_MINMAX, -1, Mat());
		normalize(hist_eql, hist_eql, 0, hist_img.rows, NORM_MINMAX, -1, Mat());

		hist_img.setTo(Scalar(0));
		hist_img_eql.setTo(Scalar(0));

		for (int i = 0; i&#60;nbins; i++) {
			line(hist_img, Point(i, histh), Point(i, cvRound(hist.at&#60;float&#62;(i))), Scalar(255, 255, 255), 1, 8, 0);
		}
		for (int i = 0; i&#60;nbins; i++) {
			line(hist_img_eql, Point(i, histh), Point(i, cvRound(hist_eql.at&#60;float&#62;(i))), Scalar(255, 255, 255), 1, 8, 0);
		}

		hist_img.copyTo(img(Rect(10, 10, nbins, histh)));
		hist_img_eql.copyTo(img_eql(Rect(10, 10, nbins, histh)));

		namedWindow("Original", WINDOW_AUTOSIZE);
		namedWindow("Equalised", WINDOW_AUTOSIZE);

		imshow("Original", img);
		imshow("Equalised", img_eql);
		if (waitKey(30) >= 0) break;
	}
	return 0;
}
					</code>
				</pre>
			</p>
			<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/equalise_video.cpp">equalise_video.cpp</a>
			<p>
			A video can better demonstrate how the program works when it has a video stream as an input:
		</p>
		<p>
			<video width="960" height="293" controls>
				<source src="./video/equalise.mp4" type="video/mp4">
				I'm sorry; your browser doesn't support HTML5 video in MP4 with H.264.
			</video>
		</p>
		<br>
		<br>
		<nav>
			<menu>
				<a href="./index.html">Home</a>
				<a href="#atividades">Activities</a>
				<a href="https://github.com/artoriuz">GitHub</a>
			</menu>
		</nav>
		<br>
		<br>
		<h3 id="movimentos">Movement Detection</h3>
		<p>
            As we know, an image is a matrix of pixels, which are arrays of subpixels containing information. A greyscale image only has a single subpixel per 
            pixel, and can only assume values between black and white. A RGB or a YUV image however, has 3 channels per pixel (3 sub-pixels) and can assume 
            different colours. In either case, an image is a matrix.
		</p>
        <p>
            A video, then, is simply a sequence of matrices. Each matrix in the sequence represents a frame, and the temporal change in values is perceived as motion. 
        </p>
        <p>
            Our objective is to be able to detect movements in a video stream, and we'll use our knowledge of what a video is in order to achieve it. Since a video is a series 
            of matrices, in order to detect movement we can compare those matrices and see if their values differ in a significant way. If they do, it's evident that there was 
            movement. If they don't, it's obvious that the image stayed still.
        </p>
        <p>
            The program simply finds the difference between the pixels of the current frame and the previous frame, creating a summation of all those values and deciding whether 
            there was movement or not depending on how high this value is. 
		</p>
		<p>
            The program also displays the difference between frames in the screen, and it's very easy to interpret the image since black values mean that the values of the frames 
            didn't change and white values mean that those values indeed changed significantly (remember the difference between frame N and frame N - 1 is what's being displayed).
		</p>
		<p>
			Without further ado, here is the program that detects movement:
		</p>
		<p>
			<pre class="prettyprint">
				<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace cv;
using namespace std;

int main(int argc, char** argv) {
	long frame_counter = 0, difference_total_value = 0;
	Mat frame, frame_past, difference;
	VideoCapture cap(argv[1]);
	
	int width = cap.get(CV_CAP_PROP_FRAME_WIDTH);
	int height = cap.get(CV_CAP_PROP_FRAME_HEIGHT);
	
	difference = Mat::zeros(height, width, CV_8UC1);

	while (true) {
		cap >> frame;
		cvtColor(frame, frame, CV_BGR2GRAY);
		
		difference_total_value = 0;
		for (int i = 0; i < frame.rows; i++) {
			for (int j = 0; j < frame.cols; j++) {
				if (frame_counter == 0) {
					difference.at&#60;uchar&#62;(i, j) = 0;
				} else {
					difference.at&#60;uchar&#62;(i, j) = abs(frame.at&#60;uchar&#62;(i, j) - frame_past.at&#60;uchar&#62;(i, j));
				}
				difference_total_value += difference.at&#60;uchar&#62;(i, j);
			}
		}

		frame_past = frame;
		frame_counter++;

		cout << "frame: " << frame_counter << " difference = " << difference_total_value << endl;

		if (difference_total_value > 150000) {
			cout << "THERE'S MOVEMENT!" << endl;
		}

		namedWindow("Difference", WINDOW_AUTOSIZE);
		imshow("Difference", difference);
		if (waitKey(30) >= 0) break;
	}
	return 0;
}
				</code>
			</pre>
		</p>
		<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/movement.cpp">movement.cpp</a>
        <p>
            For the reader, the most important part to understand here is how the difference summation is calculated. 
        </p>
        <p>
            Inside a loop, the program simply makes the difference between the current frame and the previous frame, if the frame_counter is still 0 the difference 
            is also 0 since we don't have any previous frame yet. When frame_counter is higher than 0 though, the program will calculate the difference between each pixel in
            the current frame and its counterpart in the previous frame, and then it'll add this difference into a difference summation.
        </p>
		<p>
			Here we can see what was just explained above:
		</p>
		<p>
			<pre class="prettyprint">
				<code>
difference_total_value = 0;
for (int i = 0; i < frame.rows; i++) {
	for (int j = 0; j < frame.cols; j++) {
		if (frame_counter == 0) {
			difference.at<uchar>(i, j) = 0;
		} else {
			difference.at<uchar>(i, j) = abs(frame.at<uchar>(i, j) - frame_past.at<uchar>(i, j));
		}
		difference_total_value += difference.at<uchar>(i, j);
	}
}
				</code>
			</pre>
		</p>
		<p>
			And now an example of the program running, and the terminal shouting that there was movement:
		</p>
		<p>
			<img width="960" height="451" src="./images/motion_detection/parachute.png" alt="Violet in her parachute" class="center">
		</p>
		<p>
            We can see a grey parachutist in the image, while its background remains almost completely black. This happens because while the parachutist is moving in the original video, 
            the sky (which is our background) remains still. 
		</p>
		<p>
            In this example the threshold between "no movement" and "movement" was a 
            total difference summation of 150000. The video below shows how the program works when running:
		</p>
		<p>
			<video width="960" height="600" controls>
				<source src="./video/motion_detection.mp4" type="video/mp4">
				I'm sorry; your browser doesn't support HTML5 video in MP4 with H.264.
			</video>
		</p>
		<p>
            The number of the current frame is printed at the terminal, and the total difference summation value between it and the previous frame is as well. 
            If this value is higher than our threshold, the program also prints that there was movement.
            As already explained, what we see closer to black means that there wasn't movement in that area. And what we see closer to white, means that it's moving.
		</p>
		<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
			<h3 id="lapgauss">Spacial-domain Filtering</h3>
			<p>
                To filter in the spacial domain, we appropriate ourselves from a convolution property. The property tells us that a product in the frequency domain is the equivalent
                of a convolution in the time domain. In our case "time" is simply the value variation across an axis, and we have 2 axis creating a plane.
			</p>
			<p>
				The program, as requested in the assignment should alternate between certain masks depending on the key pressed during execution. Starting from our original program  
                <a href="http://agostinhobritojr.github.io/tutoriais/pdi/exemplos/filtroespacial.cpp">filtroespacial.cpp</a>, the objective is to add the option of doing a gaussian
                filtering before applying the laplacian one. 
			</p>
			<p>
				The modified program is shown below:
			</p>
			<p>
				<pre class="prettyprint">
					<code>
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;iostream&#62;
using namespace cv;
using namespace std;

void printmask(Mat &m) {
	for (int i = 0; i&#60;m.rows; i++) {
		for (int j = 0; j&#60;m.cols; j++) {
			cout << m.at&#60;float&#62;(i, j) << ",";
		}
		cout << endl;
	}
}

void menu() {
	cout << "\npressione a tecla para ativar o filtro: \n"
		"a - calcular modulo\n"
		"m - media\n"
		"g - gauss\n"
		"v - vertical\n"
		"h - horizontal\n"
		"l - laplaciano\n"
		"esc - sair\n";
}

int main(int argvc, char** argv) {
	VideoCapture video(argv[1]); //use this for videos
	//cap = imread(argv[1]); //use this for images
	
	float media[] = { 1,1,1,1,1,1,1,1,1 };
	float gauss[] = { 1,2,1,2,4,2,1,2,1 };
	float horizontal[] = { -1,0,1,-2,0,2,-1,0,1 };
	float vertical[] = { -1,-2,-1,0,0,0,1,2,1 };
	float laplacian[] = { 0,-1,0,-1,4,-1,0,-1,0 };

	Mat cap, frame, frame32f, frameFiltered, frameFiltered2;
	Mat mask(3, 3, CV_32F), mask_gauss(3, 3, CV_32F), mask_lap(3, 3, CV_32F);
	Mat result, result1, result_lapgauss;
	double width, height, min, max;
	bool lap_gauss_aux = false, absolut = true;
	char key;

	width = video.get(CV_CAP_PROP_FRAME_WIDTH); //use this for videos
	height = video.get(CV_CAP_PROP_FRAME_HEIGHT); //use this for videos
	//width = cap.cols; //use this for images
	//height = cap.rows; //use this for images

	std::cout << "width = " << width << "\n";;
	std::cout << "height = " << height << "\n";;

	namedWindow("spacialfilter", 1);

	mask_gauss = Mat(3, 3, CV_32F, gauss);
	scaleAdd(mask_gauss, 1 / 16.0, Mat::zeros(3, 3, CV_32F), mask_gauss);
	mask_lap = Mat(3, 3, CV_32F, laplacian);

	mask = Mat(3, 3, CV_32F, media);
	scaleAdd(mask, 1 / 9.0, Mat::zeros(3, 3, CV_32F), mask);
	swap(mask, mask);

	menu();
	
	while (true) {
		video >> cap; //comment this when using images as input
		cvtColor(cap, frame, CV_BGR2GRAY);
		imshow("original", frame);
		frame.convertTo(frame32f, CV_32F);
		
		key = (char)waitKey(10);

		if (lap_gauss_aux) {
			filter2D(frame32f, frameFiltered, frame32f.depth(), mask_gauss, Point(1, 1), 0);
			filter2D(frameFiltered, frameFiltered2, frameFiltered.depth(), mask_lap, Point(1, 1), 0);

			frameFiltered2 = abs(frameFiltered2);
			frameFiltered2.convertTo(result_lapgauss, CV_8U);

			imshow("spacialfilter", result_lapgauss);

		} else {
			filter2D(frame32f, frameFiltered, frame32f.depth(), mask, Point(1, 1), 0);
			if (absolut) {
				frameFiltered = abs(frameFiltered);
			}
			frameFiltered.convertTo(result, CV_8U);
			imshow("spacialfilter", result);
		}
			
		if (key == 27) break; // esc pressed!

		switch (key) {
		case 'a':
			menu();
			absolut = !absolut;
			lap_gauss_aux = false;
			break;
		case 'm':
			menu();
			mask = Mat(3, 3, CV_32F, media);
			scaleAdd(mask, 1 / 9.0, Mat::zeros(3, 3, CV_32F), mask);
			printmask(mask);
			lap_gauss_aux = false;
			break;
		case 'g':
			menu();
			mask = mask_gauss;
			printmask(mask_gauss);
			lap_gauss_aux = false;
			break;
		case 'h':
			menu();
			mask = Mat(3, 3, CV_32F, horizontal);
			printmask(mask);
			lap_gauss_aux = false;
			break;
		case 'v':
			menu();
			mask = Mat(3, 3, CV_32F, vertical);
			printmask(mask);
			lap_gauss_aux = false;
			break;
		case 'l':
			menu();
			mask = Mat(3, 3, CV_32F, laplacian);
			printmask(mask);
			lap_gauss_aux = false;
			break;
		case 'q':
			lap_gauss_aux = true;
			break;
		default:
			break;
		}
	}
	return 0;
}							
					</code>
				</pre>
			</p>
			<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/spacialfilters.cpp">spacialfilters.cpp</a>
			<p>
                The program can be used with images or videos, simply alternating between a few set of lines already commented in the cold itself. The image 
                used as input is shown below:
			</p>
			<p>
				<img src="./images/lapgauss/erika_original.png" alt="Original Erika" class="center">	
			</p>
			<p>
				The first thing done is the convertion of the original image to greyscale: 
			</p>
			<p>
				<img src="./images/lapgauss/erika_original_greyscale.png" alt="Original Erika" class="center">	
			</p>
			<p>
				The user can alternate between different masks using the keyboard, the following list of options are available:
			</p>
			<p>
				<li>a - module</li>
				<li>m - median</li>
				<li>g - gauss</li>
				<li>v - vertical</li>
				<li>h - horizontal</li>
				<li>l - laplacian</li>
				<li>q - gauss + laplacian</li>
			</p>
			<p>
				And we can see the result of all the filters:
			</p>
			<p>
				Module:
			</p>
			<p>
				<img src="./images/lapgauss/erika_original_greyscale.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Median:
			</p>
			<p>
				<img src="./images/lapgauss/erika_median.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Gauss:
			</p>
			<p>
				<img src="./images/lapgauss/erika_gauss.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Vertical:
			</p>
			<p>
				<img src="./images/lapgauss/erika_vertical.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Horizontal:
			</p>
			<p>
				<img src="./images/lapgauss/erika_horizontal.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Laplacian:
			</p>
			<p>
				<img src="./images/lapgauss/erika_laplacian.png" alt="Original Erika" class="center">	
			</p>
			<p>
				Gauss + laplacian:
			</p>
			<p>
				<img src="./images/lapgauss/erika_lapgauss.png" alt="Original Erika" class="center">	
			</p>
			<p>
				It's easy to see that applying a gaussian blur before applying the laplacian filter makes the output cleaner and smoother.
			</p>
			<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
			<h3 id="tiltshift">Tilt-shift</h3>
			<p>
				In this section we'll do a selective focus algorithm, as if we were filming with special lenses.
			</p>
			<p>
				Let me first show the code, and then we'll learn what it does:
			</p>
			<p>
				<pre class="prettyprint">
					<code>
#include &#60;iostream&#62;
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;math.h&#62;
using namespace cv;
using namespace std;

int main(int argvc, char** argv) {
	Mat img_in = imread(argv[1]);
	Mat img_blurred = Mat::zeros(img_in.rows, img_in.cols, CV_32FC3);
	Mat img_out = Mat::zeros(img_in.rows, img_in.cols, CV_32FC3);
	Mat img_focused = Mat::zeros(img_in.rows, img_in.cols, CV_32FC3);
	Mat img_unfocused = Mat::zeros(img_in.rows, img_in.cols, CV_32FC3);
	Mat mask_focus = Mat(img_in.rows, img_in.cols, CV_32FC3, Scalar(0, 0, 0));
	Mat mask_unfocus = Mat(img_in.rows, img_in.cols, CV_32FC3, Scalar(1, 1, 1));

	imshow("input", img_in);
	waitKey();

	float unfocus_aux = 0.35; //Edit this to change how much of the original image will stay focused
	int pos_i = unfocus_aux*img_in.rows;
	int pos_f = img_in.rows - unfocus_aux*img_in.rows;
	int decay = 60; //Edit this to change how fast it'll decay

	Vec3f mask_function_value;
	for (int i = 0; i &#60; img_in.rows; i++) {
		for (int j = 0; j &#60; img_in.cols; j++) {
			mask_function_value[0] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_function_value[1] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_function_value[2] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_focus.at&#60;Vec3f&#62;(i, j) = mask_function_value;
		}
	}
	mask_unfocus = mask_unfocus - mask_focus;
	
	//This section shows the masks
	imshow("focus mask", mask_focus);
	imshow("unfocus mask", mask_unfocus);
	waitKey();
	
	
	img_in.convertTo(img_in, CV_32FC3);
	GaussianBlur(img_in, img_blurred, Size(7, 7), 0, 0);

	img_focused = img_in.mul(mask_focus);
	img_unfocused = img_blurred.mul(mask_unfocus);
	
	//This section only shows the blurred image
	img_blurred.convertTo(img_blurred, CV_8UC3);
	imshow("img_blurred", img_blurred);
	waitKey();
	
	
	img_out = img_focused + img_unfocused;
	img_out.convertTo(img_out, CV_8UC3);
	
	//This section only shows the components of the final image
	img_focused.convertTo(img_focused, CV_8UC3);
	img_unfocused.convertTo(img_unfocused, CV_8UC3);
	imshow("img_focused", img_focused);
	imshow("img_unfocused", img_unfocused);
	waitKey();
	
	imshow("output", img_out);
	waitKey();
	return 0;
}

					</code>
				</pre>
			</p>
			<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/tiltshift.cpp">tiltshift.cpp</a>
			<p>
				Now let's understand what the program does. First it simply reads its input and creates matrices that will be used later.
			</p>
			<p>
				The input image used is shown below:
			</p>
			<p>
				<img src="./images/tiltshift/img_in.png" alt="Original image" class="center">
			</p>
			<p>
				The program then, generates an auxiliar image with hyperbolic sines with the same size as the input image, we'll call this the focus mask: 
			</p>
			<p>
				<img src="./images/tiltshift/mask_focus.png" alt="focus mask" class="center">
			</p>
			<p>
				Using the image previously generated, we generate its inverse (the "unfocus mask") with a simple subtraction from a white matrix:
			</p>
			<p>
				<img src="./images/tiltshift/mask_unfocus.png" alt="unfocus mask" class="center">
			</p>
			<p>
				We then create a blurred version of the original image through a gaussian blur filter, this image will be used as our unfocused version of the original image: 
			</p>
			<p>
				<img src="./images/tiltshift/img_blurred.png" alt="blurred image" class="center">
			</p>
			<p>
				The focus mask then multiplies the original image, and we get the focused portion of our final image:
			</p>
			<p>
				<img src="./images/tiltshift/img_focused.png" alt="focused image" class="center">
			</p>
			<p>
				The same is done with the blurred version and the unfocus mask:
			</p>
			<p>
				<img src="./images/tiltshift/img_unfocused.png" alt="unfocused image" class="center">
			</p>
			<p>
				Finally, to arrive at our intended result, we simply sum the focused image and the unfocused one:
			</p>
			<p>
				<img src="./images/tiltshift/img_out.png" alt="output image" class="center">
			</p>
			<p>
				We can see that, when compared to the original image, the output image's vertical extremities seem to be out of focus. However, the centre of it stayed sharp and clear.
			</p>
			<video loop autoplay>
				<source src="./video/tiltshift.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video> 
			<p>
                We can utilise the same effect to create a video that looks like it was filmed from a miniature, and make it more convincing including a frame skip effect
                which is common on those videos to fake motion.
			</p>
			<p>
				In order to be used with videos the code needs to be modified, and we can see the modified version below:
			</p>
			<p>
				<pre class="prettyprint">
					<code>
#include &#60;iostream&#62;
#include &#60;opencv2/opencv.hpp&#62;
#include &#60;math.h&#62;
using namespace cv;
using namespace std;

int main(int argvc, char** argv) {
	VideoCapture video(argv[1]);
	Mat img_in, img_discard;
	int cols = video.get(CV_CAP_PROP_FRAME_WIDTH);
	int rows = video.get(CV_CAP_PROP_FRAME_HEIGHT);

	Mat img_blurred = Mat::zeros(rows, cols, CV_32FC3);
	Mat img_out = Mat::zeros(rows, cols, CV_32FC3);
	Mat img_focused = Mat::zeros(rows, cols, CV_32FC3);
	Mat img_unfocused = Mat::zeros(rows, cols, CV_32FC3);
	Mat mask_focus = Mat(rows, cols, CV_32FC3, Scalar(0, 0, 0));
	Mat mask_unfocus = Mat(rows, cols, CV_32FC3, Scalar(1, 1, 1));

	int frame_skip = 2; //Sets how many frames to skip between frames
	int pass_frame = 0;

	float unfocus_aux = 0.35; //Edit this to change how much of the original image will stay focused
	int pos_i = unfocus_aux*rows;
	int pos_f = rows - unfocus_aux*rows;
	int decay = 60; //Edit this to change how fast it'll decay

	Vec3f mask_function_value;

	for (int i = 0; i &#60; rows; i++) {
		for (int j = 0; j &#60; cols; j++) {
			mask_function_value[0] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_function_value[1] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_function_value[2] = (tanh((float(i - pos_i) / decay)) - tanh((float(i - pos_f) / decay))) / 2;
			mask_focus.at&#60;Vec3f&#62;(i, j) = mask_function_value;
		}
	}
	mask_unfocus = mask_unfocus - mask_focus;

	//This section shows the masks
	imshow("focus mask", mask_focus);
	imshow("unfocus mask", mask_unfocus);
	
	while (true) {
		if (pass_frame == 0) {
			video &#62;&#62; img_in;
		} else {
			video &#62;&#62; img_discard;
		}
		
		img_in.convertTo(img_in, CV_32FC3);
		GaussianBlur(img_in, img_blurred, Size(7, 7), 0, 0);

		img_focused = img_in.mul(mask_focus);
		img_unfocused = img_blurred.mul(mask_unfocus);

		//This section only shows the blurred image
		img_blurred.convertTo(img_blurred, CV_8UC3);
		imshow("img_blurred", img_blurred);
		//waitKey();

		img_out = img_focused + img_unfocused;
		img_out.convertTo(img_out, CV_8UC3);

		//This section only shows the components of the final image
		img_focused.convertTo(img_focused, CV_8UC3);
		img_unfocused.convertTo(img_unfocused, CV_8UC3);
		imshow("img_focused", img_focused);
		imshow("img_unfocused", img_unfocused);
		//waitKey();

		if (pass_frame == 0) {
			pass_frame = frame_skip;
		}
		else {
			pass_frame -= 1;
		}

		imshow("output", img_out);
		if (waitKey(30) &#62;= 0) break;
	}
	return 0;
}
					</code>
				</pre>
			</p>
			<a href="https://github.com/Artoriuz/OpenCV-Examples/blob/master/tiltshift_video.cpp">tiltshift_video.cpp</a>
			<p>
                As we can see the skeleton of the code remains the same, and the divergence to the original code is related to acquiring frames and passing those frames to 
                the matrices that will process the information and generate our intended result.
			</p>
			<p>
				Since we'll now run the program with a video as its input, it's important to know how the original video looks like:
			</p>
			<p>
				<video controls>
					<source src="./video/tilt_shift_video_original.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video> 
			</p>
			<p>
                In this example, since the video's resolution is the same as the image's resolution in the previous example, our masks will look identical 
                to those already shown before. The frame skipping logic was set to skip 2 frames every frame that's passed to the algorithm. This number needs to be tweaked 
                in order to create a better miniature effect.
			</p>
			<p>
                We can see the output video below. Due to the fact that we're throwing 2/3 of the frames away, but holdind the already passed
                frames until a new frame can be passed, it looks slower than the original: 
			</p>
			<p>
				<video controls>
					<source src="./video/tilt_shift_video_output.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video> 
			</p>
			<p>
				We can however, "fix" the motion speed problem by encoding the video with a lower frame persistance time:
			</p>
			<p>
				<video controls>
					<source src="./video/tilt_shift_video_output_accelerated.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video> 
			</p>
			<br>
			<br>
			<nav>
				<menu>
					<a href="./index.html">Home</a>
					<a href="#atividades">Activities</a>
					<a href="https://github.com/artoriuz">GitHub</a>
				</menu>
			</nav>
			<br>
			<br>
		</section>



		<footer>

		</footer>
</body>

</html>